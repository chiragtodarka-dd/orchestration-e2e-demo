import inspect
import os
import sys
import importlib.util
from typing import Type, Dict, Any, Optional, List
from datetime import datetime
from dataclasses import fields, is_dataclass # For generated operator

# Need to ensure function_abstract can be found
# This assumes the generator script is run from the project root or PYTHONPATH is set appropriately.
# For dynamic loading, the function_abstract.py needs to be discoverable.
# One way is to ensure this script's parent directory's parent is in sys.path if running this script directly.
CURRENT_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT_FOR_FUNCTION_ABSTRACT = os.path.abspath(os.path.join(CURRENT_SCRIPT_DIR, "../../..")) # Adjust if function_abstract moves
if PROJECT_ROOT_FOR_FUNCTION_ABSTRACT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT_FOR_FUNCTION_ABSTRACT)

from airflow.models import BaseOperator
# from airflow.utils.decorators import apply_defaults # Removed in Airflow 3.0
from orchestration.function.function_abstract import Function, ExecutionContext # Adjusted path

GENERATOR_COMMENT = "# This file is auto-generated by function_to_operator_generator.py\n# Changes will be overwritten.\n"

class FunctionOperatorGenerator:
    """
    Utility class to generate Airflow operators from Function implementations.
    """
    
    def __init__(self, function_class: Type[Function]):
        """
        Initialize the generator.
        
        Args:
            function_class: The loaded Function implementation class.
        """
        self.function_class = function_class
        self.function_module_path = function_class.__module__
        base_name = function_class.__name__.replace('Function', '')
        if not base_name: # Handle case where class is just named "Function"
            base_name = function_class.__name__
        self.operator_name = f"{base_name}Operator"
        self.operator_file_name = f"{base_name.lower()}operator.py"
    
    def _get_function_init_params(self) -> List[inspect.Parameter]:
        """
        Extract parameters from the Function class's __init__ method,
        excluding 'self' and 'context'.
        """
        init_signature = inspect.signature(self.function_class.__init__)
        func_params = []
        for name, param in init_signature.parameters.items():
            if name not in ['self', 'context']:
                func_params.append(param)
        return func_params
    
    def _generate_operator_class_code(self) -> str:
        """
        Generate the Airflow operator class code as a string.
        """
        init_params = self._get_function_init_params()
        
        # Operator __init__ parameters string for @dataclass fields
        # and for __post_init__ to collect them into function_params
        operator_init_fields_str = ""
        function_constructor_args = [] # For passing to Function constructor
        
        for param in init_params:
            param_name = param.name
            param_type_hint = "Any"
            if param.annotation != inspect.Parameter.empty:
                # Try to get a string representation of the type hint
                if hasattr(param.annotation, '__name__'):
                    param_type_hint = param.annotation.__name__
                elif hasattr(param.annotation, '_name') and param.annotation._name: # For typing.Optional, etc.
                     param_type_hint = str(param.annotation).replace('typing.','')
                else:
                    param_type_hint = str(param.annotation).replace('typing.','') # Fallback for complex types
            
            if param.default == inspect.Parameter.empty:
                operator_init_fields_str += f"    {param_name}: {param_type_hint}\n"
            else:
                operator_init_fields_str += f"    {param_name}: {param_type_hint} = {repr(param.default)}\n"
            function_constructor_args.append(f"{param_name}=self.{param_name}")

        # Add secret_key as a specific parameter for the operator
        operator_init_fields_str += "    secret_key: Optional[str] = None\n"
        # Add task_id and other BaseOperator params that @dataclass doesn't handle automatically if needed
        # However, BaseOperator handles task_id, owner, etc. when super().__init__() is called.
        # We will rely on passing *args, **kwargs for other BaseOperator params.

        function_params_dict_str = "{\n"
        for param in init_params:
            function_params_dict_str += f"            '{param.name}': self.{param.name},\n"
        # Remove last comma and add closing brace
        if init_params: # only if there are params
            function_params_dict_str = function_params_dict_str.rstrip(",\n") + "\n        }"
        else:
             function_params_dict_str = "{}" # Empty dict if no params

        # Construct the import path for the function class
        # self.function_module_path is like orchestration.function.postgres_sql_function
        # from {self.function_module_path} import {self.function_class.__name__}

        operator_code = f'''{GENERATOR_COMMENT}
import os # For potential path resolutions in generated operator if needed
from typing import Any, Dict, Optional, List # Add List if any param hints require it
from datetime import datetime
from dataclasses import dataclass, field # field might be useful for more complex defaults

from airflow.models import BaseOperator
from orchestration.function.function_abstract import ExecutionContext # Path to ExecutionContext
from {self.function_module_path} import {self.function_class.__name__}

@dataclass(kw_only=True) # kw_only=True makes all @dataclass fields keyword-only
class {self.operator_name}(BaseOperator):
    """
    Airflow operator for {self.function_class.__name__}.
    This operator is auto-generated.
    """
    
{operator_init_fields_str}
    # BaseOperator parameters like task_id, owner etc. will be passed via **kwargs to super()
    # and are not explicitly defined as dataclass fields here to avoid conflicts.

    def __post_init__(self, **kwargs): # Accept **kwargs for BaseOperator
        # Initialize BaseOperator with all *args and **kwargs not consumed by dataclass
        # task_id is typically passed in kwargs
        super().__init__(**kwargs) 
        # self.function_params are the specific arguments for the Function's __init__
        self.function_params: Dict[str, Any] = {function_params_dict_str}
    
    def execute(self, context: Dict[str, Any]) -> Any:
        """
        Execute the function with the given Airflow context.
        """
        print(f"Executing operator {self.operator_name} for task {{self.task_id}}")
        
        exec_context = ExecutionContext(
            execution_date=context.get('execution_date', context.get('data_interval_start')), # For Airflow 2.x vs 3.x
            task_id=self.task_id,
            dag_id=self.dag_id,
            run_id=context.get('run_id'),
            params=context.get('params', {{}}),
            secret_key=self.secret_key
        )
        
        print(f"Initializing function {self.function_class.__name__} with params: {{self.function_params}} and secret_key: {{self.secret_key}}")
        function_instance = {self.function_class.__name__}(
            context=exec_context,
            **self.function_params
        )
        
        try:
            print(f"Calling pre_execute for {{self.task_id}}")
            function_instance.pre_execute()
            print(f"Calling execute for {{self.task_id}}")
            result = function_instance.execute()
            print(f"Calling post_execute for {{self.task_id}}")
            function_instance.post_execute()
            print(f"Calling on_success for {{self.task_id}}")
            function_instance.on_success()
            return result
        except Exception as e:
            print(f"Exception in {{self.task_id}}: {{e}}. Calling on_failure.")
            function_instance.on_failure()
            raise
'''
        return operator_code
    
    def generate_operator_file(self, output_dir: str) -> str:
        """
        Generate the operator file in the specified directory.
        """
        os.makedirs(output_dir, exist_ok=True)
        operator_code = self._generate_operator_class_code()
        output_path = os.path.join(output_dir, self.operator_file_name)
        
        with open(output_path, 'w') as f:
            f.write(operator_code)
        print(f"Generated Airflow operator: {output_path}")
        return output_path

def load_function_class_from_file(file_path: str) -> Optional[Type[Function]]:
    """
    Load a Function subclass from a Python file.
    Assumes the file contains one class inheriting from Function (excluding Function itself).
    """
    if not os.path.exists(file_path):
        print(f"Error: File not found at {file_path}")
        return None
        
    module_name = os.path.splitext(os.path.basename(file_path))[0]
    
    # Temporarily add the file's directory to sys.path for the import to work relative to project structure
    file_dir = os.path.dirname(os.path.abspath(file_path))
    original_sys_path = list(sys.path)
    if file_dir not in sys.path:
        sys.path.insert(0, file_dir)
        
    # For imports within the loaded module (e.g., importing function_abstract)
    # Assuming orchestration.function.function_abstract structure relative to project root
    project_root = os.path.abspath(os.path.join(file_dir, "../../")) # Adjust if file_path is deeper
    if project_root not in sys.path:
        sys.path.insert(1, project_root) # Insert after file_dir to prioritize local module files

    function_class = None
    try:
        spec = importlib.util.spec_from_file_location(module_name, file_path)
        if spec is None or spec.loader is None:
            print(f"Error: Could not create module spec from {file_path}")
            return None
        
        module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(module) # Execute the module to define its contents
        
        for name, obj in inspect.getmembers(module):
            if inspect.isclass(obj) and issubclass(obj, Function) and obj is not Function:
                if function_class is not None:
                    print(f"Warning: Multiple Function subclasses found in {file_path}. Using first one: {function_class.__name__}")
                    break
                function_class = obj
        
        if function_class is None:
            print(f"Error: No class inheriting from Function found in {file_path}")
            return None
            
        return function_class
    except Exception as e:
        print(f"Error loading function class from {file_path}: {e}")
        return None
    finally:
        sys.path = original_sys_path # Restore original sys.path

def main():
    """
    Main entry point for the script.
    Parses command line arguments for input function file and output directory.
    """
    if len(sys.argv) != 3:
        print("Usage: python function_to_operator_generator.py <absolute_path_to_function_file.py> <absolute_path_to_output_dir>")
        sys.exit(1)
    
    function_file_path = sys.argv[1]
    output_dir = sys.argv[2]

    if not os.path.isabs(function_file_path):
        print(f"Error: Function file path must be absolute. Provided: {function_file_path}")
        sys.exit(1)
    if not os.path.isabs(output_dir):
        print(f"Error: Output directory path must be absolute. Provided: {output_dir}")
        sys.exit(1)

    print(f"Attempting to load function from: {function_file_path}")
    function_class = load_function_class_from_file(function_file_path)
    
    if function_class:
        print(f"Successfully loaded function class: {function_class.__name__}")
        generator = FunctionOperatorGenerator(function_class)
        generator.generate_operator_file(output_dir)
    else:
        print("Could not generate operator due to previous errors.")
        sys.exit(1)

if __name__ == "__main__":
    main() 